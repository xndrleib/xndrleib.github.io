[{"content":"An In-Depth Overview of the Simplex Method Abstract\nThis document presents an in-depth exposition of the simplex method for solving linear programs. We discuss fundamental concepts such as basic feasible solutions (BFS), the notion of adjacency between extreme points, and the nondegeneracy assumption. Both the primal and the dual simplex methods are described from a matrix–theoretic viewpoint with illustrative examples. Finally, we present a worst-case efficiency analysis under a basic value distribution assumption.\nIntroduction The simplex method is one of the most widely used algorithms for solving linear programming (LP) problems. The method iteratively moves from one basic feasible solution (or extreme point) of the feasible region to an adjacent one, improving the objective function value at each step until optimality is reached. Theoretical results assure that it suffices to search only among basic feasible solutions, and duality theory provides termination criteria and optimality certificates. This document outlines both the primal and dual simplex methods, explains the mechanics of basis changes, and concludes with an analysis of worst-case performance.\nBasic Concepts and Notation Consider a linear program in standard form:\n$$ \\begin{aligned} \\text{minimize} \\quad \u0026 \\mathbf{c}^\\top \\mathbf{x} \\\\ \\text{subject to} \\quad \u0026 \\mathbf{A} \\mathbf{x} = \\mathbf{b}, \\\\ \u0026 \\mathbf{x} \\ge \\mathbf{0}, \\end{aligned} $$where $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is of full row rank, $\\mathbf{b}\\in\\mathbb{R}^m$, and $\\mathbf{c}\\in\\mathbb{R}^n$.\nA basic feasible solution (BFS) is obtained by choosing a basis (a set of $m$ linearly independent columns of $\\mathbf{A}$), solving for the corresponding basic variables, and setting the remaining $n-m$ nonbasic variables to zero. When the current basic solution is unique (i.e., all basic variables are strictly positive), it is said to be nondegenerate.\nIntuitive Explanation of a Basic Feasible Solution To build intuition, consider a simple linear program:\n$$ \\begin{aligned} \\text{minimize} \\quad \u0026 x_1 + 2x_2, \\\\ \\text{subject to} \\quad \u0026 x_1 + x_2 = 3, \\\\ \u0026 x_1,\\, x_2 \\ge 0. \\end{aligned} $$Here, the feasible region is a line segment connecting the points $(0,3)$ and $(3,0)$. These endpoints correspond to the extreme points (or BFSs) of the feasible region. Selecting one endpoint (i.e., setting one variable to zero and solving for the other) illustrates the idea of a basis in a simple, tangible manner. This example shows that by choosing an appropriate set of variables (the basis), we determine a unique solution that is critical to the simplex method.\nAdjacency and Basis Changes Adjacency of Basic Feasible Solutions Definition (Adjacent BFS):\nTwo basic feasible solutions are said to be adjacent (or corresponding to adjacent extreme points) if and only if their bases differ in exactly one column.\nIn other words, a new BFS can be generated from an existing one by replacing one basic variable with a nonbasic variable.\nNondegeneracy Assumption For many arguments it is convenient to assume:\nNondegeneracy Assumption: Every basic feasible solution of the LP is nondegenerate.\nUnder this assumption, when a new variable enters the basis, one of the basic variables decreases strictly to zero, ensuring a proper pivot.\nDetermination of the Leaving Variable Let the current BFS be partitioned as\n$$ \\mathbf{x} = \\begin{pmatrix} \\mathbf{x}_B \\\\ \\mathbf{x}_N \\end{pmatrix}, $$with basis matrix $\\mathbf{B}$ and nonbasic matrix $\\mathbf{N}$. Suppose a nonbasic variable $x_e$ (with column $\\mathbf{a}_e$) is chosen to enter the basis. Then the equality\n$$ \\mathbf{b} = \\mathbf{B}\\mathbf{x}_B + \\mathbf{a}_e x_e $$can be rewritten as\n$$ \\mathbf{x}_B = \\overline{\\mathbf{a}}_0 - x_e \\, \\overline{\\mathbf{a}}_e, $$where\n$$ \\overline{\\mathbf{a}}_0 = \\mathbf{B}^{-1}\\mathbf{b} \\quad \\text{and} \\quad \\overline{\\mathbf{a}}_e = \\mathbf{B}^{-1}\\mathbf{a}_e. $$To maintain feasibility ($\\mathbf{x}_B \\ge \\mathbf{0}$), we must have\n$$ x_e \\le \\min_{i:\\,\\overline{a}_{ie}\u003e0} \\frac{(\\overline{\\mathbf{a}}_0)\\_i}{\\overline{a}\\_{ie}}. $$The index achieving this minimum determines the leaving variable.\nConic Combination Interpretation A feasible solution for the system $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ with $\\mathbf{x} \\ge \\mathbf{0}$ represents a conic combination of the columns of $\\mathbf{A}$. In a BFS, only $m$ columns (the basic ones) are used with positive weights. This interpretation can be visualized in the requirements space (the space spanned by the columns of $\\mathbf{A}$). For example, if $\\mathbf{b}$ lies between columns $\\mathbf{a}_1$ and $\\mathbf{a}_2$ (in the appropriate sense), then a BFS with positive weights on $\\mathbf{a}_1$ and $\\mathbf{a}_2$ exists.\nFigure: Constraint representation in requirements space.\nThe Primal Simplex Method Determining an Optimal Feasible Solution Assume that the basis matrix $\\mathbf{B}$ consists of the first $m$ columns of $\\mathbf{A}$ and partition the data accordingly:\n$$ \\mathbf{A} = [\\mathbf{B}\\,|\\,\\mathbf{N}],\\quad \\mathbf{x} = \\begin{pmatrix}\\mathbf{x}_B \\\\ \\mathbf{x}_N \\end{pmatrix},\\quad \\mathbf{c}^\\top = [\\mathbf{c}_B^\\top \\,\\, \\mathbf{c}_N^\\top]. $$If the current BFS is given by\n$$ \\mathbf{x}_B = \\overline{\\mathbf{a}}_0 = \\mathbf{B}^{-1}\\mathbf{b}, \\quad \\mathbf{x}_N = \\mathbf{0}, $$then any feasible solution satisfies\n$$ \\mathbf{x}_B = \\mathbf{B}^{-1}\\mathbf{b} - \\mathbf{B}^{-1}\\mathbf{N}\\mathbf{x}_N. $$Substituting into the cost function gives\n$$ z = \\mathbf{c}^\\top \\mathbf{x} = \\mathbf{c}_B^\\top \\overline{\\mathbf{a}}_0 + \\left(\\mathbf{c}_N^\\top - \\mathbf{c}_B^\\top \\mathbf{B}^{-1}\\mathbf{N}\\right)\\mathbf{x}_N. $$Defining the simplex multipliers as\n$$ \\mathbf{y}^\\top = \\mathbf{c}_B^\\top \\mathbf{B}^{-1} $$and the reduced cost vector as\n$$ \\mathbf{r}_N^\\top = \\mathbf{c}_N^\\top - \\mathbf{y}^\\top \\mathbf{N}, $$we have\n$$ z = z_0 + \\mathbf{r}_N^\\top \\mathbf{x}_N,\\quad \\text{where } z_0 = \\mathbf{c}_B^\\top \\overline{\\mathbf{a}}_0. $$A negative component of $\\mathbf{r}_N$ indicates that increasing the corresponding nonbasic variable will improve the objective.\nImprovement and Optimality Conditions Theorem (Improvement of a Basic Feasible Solution):\nSuppose that in a nondegenerate BFS the current objective value is $z_0$ and for some nonbasic variable $x_j$ the reduced cost $r_j \u003c 0$. Then increasing $x_j$ from zero produces a new feasible solution with objective value $z \u003c z_0$. Moreover, if the associated pivot yields a new BFS, the new solution is strictly better. If no basic variable can leave the basis (i.e., all entries in $\\overline{\\mathbf{a}}_e$ are nonpositive), then the feasible region is unbounded and the objective can be decreased without bound.\nProof:\nSince $r_j \u003c 0$, any increase $x_j' \u003e 0$ results in\n$$ z - z_0 = r_j\\, x_j' \u003c 0. $$Feasibility is maintained by adjusting the basic variables as\n$$ \\mathbf{x}_B = \\overline{\\mathbf{a}}_0 - x_j'\\,\\overline{\\mathbf{a}}_j, $$and increasing $x_j'$ until the first basic variable hits zero. This change yields a new BFS with a strictly lower objective value.\nTheorem (Optimality Condition):\nIf all reduced costs satisfy $r_j \\ge 0$ for every nonbasic variable $x_j$, then the current BFS is optimal.\nProof:\nIf $r_j \\ge 0$ for all nonbasic variables, then any feasible change (which involves increasing a nonbasic variable from zero) would not lower the objective function. By duality and complementary slackness, the current solution is optimal.\nThe Revised Primal Simplex Procedure The primal simplex algorithm may be summarized as follows:\nStep 0 (Initialization):\nStart with a BFS corresponding to a basis $\\mathbf{B}$ with $\\mathbf{x}_B = \\overline{\\mathbf{a}}_0 = \\mathbf{B}^{-1}\\mathbf{b} \\ge \\mathbf{0}$ and $\\mathbf{x}_N = \\mathbf{0}$.\nStep 1 (Compute Multipliers and Reduced Costs):\nCalculate $\\mathbf{y}^\\top = \\mathbf{c}_B^\\top \\mathbf{B}^{-1}$ and $\\mathbf{r}_N^\\top = \\mathbf{c}_N^\\top - \\mathbf{y}^\\top \\mathbf{N}$. If $\\mathbf{r}_N \\ge \\mathbf{0}$, stop—the current solution is optimal.\nStep 2 (Choose Entering Variable):\nSelect a nonbasic index $e$ such that $r_e \u003c 0$. Compute $\\overline{\\mathbf{a}}_e = \\mathbf{B}^{-1}\\mathbf{a}_e$.\nStep 3 (Determine Leaving Variable):\nIf all entries of $\\overline{\\mathbf{a}}_e \\le 0$, then the problem is unbounded. Otherwise, compute the ratios\n$$ \\varepsilon_i = \\frac{(\\overline{\\mathbf{a}}_{0})_{i}}{\\overline{a}_{ie}} \\quad \\text{for } \\overline{a}_{ie} \u003e 0, $$and let $o = \\arg\\min_i \\varepsilon_i$. The $o$th basic variable leaves the basis.\nStep 4 (Pivot and Update):\nUpdate the basis (or its factorization) to reflect the exchange of $\\mathbf{a}_o$ and $\\mathbf{a}_e$. Compute the new BFS $\\overline{\\mathbf{a}}_0 = \\mathbf{B}^{-1}\\mathbf{b}$. Return to Step 1.\nRemark: Avoiding Cycling with Bland\u0026rsquo;s Rule An important refinement to the simplex method is Bland\u0026rsquo;s rule. This rule selects the entering and leaving variables by choosing the one with the smallest index that satisfies the improvement condition. Bland\u0026rsquo;s rule prevents cycling, ensuring convergence even in the presence of degeneracy. Although it may not always produce the fastest improvement in objective value, its use is valuable in both theoretical analysis and practical implementations to guarantee termination.\nExample: Primal Simplex Procedure Example (Primal Simplex Procedure Illustration):\nConsider the LP:\n$$ \\begin{array}{rl} \\text{minimize} \u0026 18x_1+12x_2+2x_3+6x_4, \\\\ \\text{subject to} \u0026 3x_1 + x_2 - 2x_3 + x_4 = 2, \\\\ \u0026 x_1+3x_2 - x_4 = 2, \\\\ \u0026 x_j \\ge 0,\\quad j=1,\\dots,4. \\end{array} $$Assume the initial basis is chosen with columns $\\mathbf{a}_1$ and $\\mathbf{a}_3$ (so that $x_2$ and $x_4$ are nonbasic). One carries out the following:\nStep 1: Compute $\\mathbf{B}$, its inverse, and the initial basic solution $\\overline{\\mathbf{a}}_0$. Step 2: Determine the simplex multipliers $\\mathbf{y}^\\top$ and the reduced cost vector $\\mathbf{r}_N^\\top$. In this example, suppose $r_2 \u003c 0$ so that $x_2$ is chosen as the entering variable. Step 3: Compute $\\overline{\\mathbf{a}}_2 = \\mathbf{B}^{-1}\\mathbf{a}_2$ and then the ratio $\\overline{\\mathbf{a}}_0 / \\overline{\\mathbf{a}}_2$ to select the leaving variable. Step 4: Update the basis, recalculate $\\mathbf{B}^{-1}$ and $\\overline{\\mathbf{a}}_0$, and continue. After a finite number of iterations, if all reduced costs are nonnegative, the current BFS is optimal.\nThe Dual Simplex Method In some cases the starting basis is not feasible for the primal but the corresponding dual solution is feasible. The dual simplex method exploits this situation by iterating on a dual feasible basis until primal feasibility is attained.\nDual Simplex Procedure Assume the current basis $\\mathbf{B}$ gives a primal solution $\\overline{\\mathbf{a}}_0 = \\mathbf{B}^{-1}\\mathbf{b}$ that is not feasible (i.e., some components are negative) but the dual multipliers $\\mathbf{y}^\\top = \\mathbf{c}_B^\\top \\mathbf{B}^{-1}$ yield a dual feasible solution with $\\mathbf{r}_N = \\mathbf{c}_N^\\top - \\mathbf{y}^\\top \\mathbf{N} \\ge \\mathbf{0}$.\nThe dual simplex algorithm proceeds as follows:\nStep 0 (Initialization):\nStart with a basis $\\mathbf{B}$ such that the dual is feasible.\nStep 1 (Select Leaving Variable):\nIdentify an index $o$ for which $(\\overline{\\mathbf{a}}_0)_o \u003c 0$ (choose the most negative component).\nStep 2 (Compute Dual Row and Determine Entering Variable):\nCompute the dual row $\\overline{\\mathbf{y}}^\\top = \\mathbf{e}_o^\\top \\mathbf{B}^{-1}$ and then $\\overline{\\mathbf{a}}^o = \\overline{\\mathbf{y}}^\\top \\mathbf{N}$. If $\\overline{\\mathbf{a}}^o \\ge 0$, the problem is unbounded. Otherwise, for each index $j$ with $\\overline{a}^o_j \u003c 0$, compute the ratio\n$$ \\rho_j = \\frac{r_j}{-\\overline{a}^o_j}, $$and let $e = \\arg\\min_j \\rho_j$. Then, column $e$ enters the basis.\nStep 3 (Update):\nPivot to update $\\mathbf{B}$ (or its factorization) and update $\\overline{\\mathbf{a}}_0$, $\\mathbf{y}$, and $\\mathbf{r}_N$. Return to Step 1.\nPractical Considerations for the Dual Simplex Method In many real-world scenarios, the dual simplex method offers a computational advantage, especially when the initial basis is nearly optimal for the dual but not feasible for the primal. It is particularly effective in reoptimization contexts, such as when small changes are made to a previously solved LP. However, similar to the primal simplex method, care must be taken to manage numerical precision and to select variables judiciously to ensure rapid convergence.\nExample: Dual Simplex Procedure Example (Dual Simplex Procedure Illustration):\nSuppose we start with a basis\n$$ \\mathbf{B} = \\begin{pmatrix} 1 \u0026 -2 \\\\ 3 \u0026 0 \\end{pmatrix} $$with corresponding inverse $\\mathbf{B}^{-1}$ and compute the primal solution $\\overline{\\mathbf{a}}_0$. If one component of $\\overline{\\mathbf{a}}_0$ is negative (say, the second component), then that row is used to compute $\\overline{\\mathbf{y}}^\\top$ and the dual row $\\overline{\\mathbf{a}}^o$. By forming the ratios of the reduced costs to $-\\overline{a}^o_j$ for nonbasic indices, the entering variable is determined. After pivoting, if the new primal solution becomes feasible, then optimality is reached.\nThe Primal-Dual Algorithm In the primal-dual method the primal and dual problems are solved simultaneously. Given a dual feasible solution $\\mathbf{y}$, define the index set\n$$ P = \\{ j : \\mathbf{y}^\\top \\mathbf{a}_j = c_j \\}. $$Then the associated restricted primal is\n$$ \\begin{aligned} \\text{minimize} \\quad \u0026 \\mathbf{1}^\\top \\mathbf{u} \\\\ \\text{subject to} \\quad \u0026 \\mathbf{A} \\mathbf{x} + \\mathbf{u} = \\mathbf{b},\\\\ \u0026 \\mathbf{x} \\ge \\mathbf{0}, \\quad x_j = 0 \\; \\text{for } j \\notin P,\\\\ \u0026 \\mathbf{u} \\ge \\mathbf{0}. \\end{aligned} $$Its dual (the restricted dual) provides an improving direction for $\\mathbf{y}$. The following theorem establishes the optimality of a pair of solutions.\nTheorem (Primal-Dual Optimality):\nSuppose $\\mathbf{y}$ is feasible for the dual and $(\\mathbf{x},\\mathbf{u}=\\mathbf{0})$ is feasible (and optimal) for the associated restricted primal above. Then $\\mathbf{x}$ and $\\mathbf{y}$ are optimal for the original primal and dual problems, respectively.\nProof:\nSince $\\mathbf{x}$ is feasible for the primal, and by complementary slackness the equality\n$$ \\mathbf{c}^\\top \\mathbf{x} = \\mathbf{y}^\\top \\mathbf{A}\\mathbf{x} = \\mathbf{y}^\\top \\mathbf{b} $$holds, optimality follows by strong duality.\nThe primal-dual method iteratively adjusts the dual solution $\\mathbf{y}$ (using an appropriate step size) until the restricted primal attains a zero optimal value, indicating that complementary slackness holds.\nEfficiency Analysis of the Simplex Method While extensive computational experience shows that the simplex method is efficient on most practical problems, worst-case examples exist. In this section we briefly outline a worst-case iteration bound under a basic value distribution property.\nBasic Value Distribution Definition (Basic Value Distribution):\nA BFS $\\mathbf{x}_B$ is said to satisfy the $(\\Delta,\\delta)$ property if\n$$ \\mathbf{1}^\\top \\mathbf{x}_B \\le \\Delta \\quad \\text{and} \\quad \\min_i (\\mathbf{x}_B)_i \\ge \\delta, $$for some positive constants $\\Delta$ and $\\delta$. This condition implies nondegeneracy and controls the spread of the basic variable values.\nGeometric Reduction of the Objective Gap Lemma:\nLet $\\mathbf{x}^k$ be the current BFS with objective value $z^k = \\mathbf{c}^\\top \\mathbf{x}^k$ and let $z^*$ denote the optimal objective value. Under the $(\\Delta,\\delta)$ property, the next BFS $\\mathbf{x}^{k+1}$ satisfies\n$$ \\frac{z^{k+1} - z^{*}}{z^{k} - z^{*}} \\le 1 - \\frac{\\delta}{\\Delta}. $$Proof:\nLet $r_e^k \u003c 0$ be the most negative reduced cost at iteration $k$. Then the gap $z^k - z^*$ can be bounded by\n$$ z^k - z^* \\le |r_e^k| \\Delta. $$Since the change in objective is at most $r_e^k \\delta$, we obtain\n$$ z^{k+1} - z^* \\le z^k - z^* - |r_e^k|\\delta \\le \\left(1-\\frac{\\delta}{\\Delta}\\right)(z^k-z^*). $$Elimination of Nonoptimal Variables Lemma:\nLet $\\mathbf{x}^0$ be a nonoptimal BFS with basis $B^0$. Then there exists a basic variable $x_{j^0}$ (with $j^0\\in B^0$ but not in the optimal basis $B^*$) that will never reappear in any BFS generated after\n$$ K := \\left\\lceil \\frac{\\Delta}{\\delta} \\log\\left(\\frac{m \\Delta}{\\delta}\\right) \\right\\rceil $$iterations.\nProof:\nSince the initial gap \\(z^0 - z^*\\) is positive and at least one basic variable not in the optimal basis must have a corresponding reduced cost $r^*_j$ bounded away from zero, after $K$ iterations (using the geometric reduction from the previous lemma) the contribution of that variable becomes negligible. A contradiction then shows that such a variable must leave the basis permanently.\nWorst-Case Iteration Bound Theorem:\nUnder the assumption that every BFS generated satisfies the $(\\Delta,\\delta)$ property, the simplex method terminates in at most\n$$ \\left\\lceil \\frac{(n-m)\\Delta}{\\delta} \\cdot \\log\\left(\\frac{m\\Delta}{\\delta}\\right) \\right\\rceil $$iterations.\nProof:\nSince there are at most $n-m$ nonoptimal basic variables that can be eliminated (by the previous lemma) and each elimination requires at most $\\left\\lceil \\frac{\\Delta}{\\delta}\\log\\left(\\frac{m\\Delta}{\\delta}\\right) \\right\\rceil$ iterations (by the geometric reduction lemma), the total number of iterations is bounded by the stated expression.\nDiscussion on Efficiency Analysis While the above theorem provides a worst-case iteration bound under the $(\\Delta,\\delta)$ property, it is important to emphasize that such worst-case scenarios are rarely encountered in practice. Typically, the simplex method converges in significantly fewer iterations than the bound suggests.\nPractical Implications The parameter ratio $\\delta/\\Delta$ plays a crucial role in the convergence speed. A higher ratio indicates that the basic variables are well-distributed (i.e., less degeneracy), which typically leads to faster convergence. This insight encourages the use of preconditioning or scaling techniques in linear programming solvers to improve the performance of the simplex algorithm in practice.\nSummary and Concluding Remarks In summary, we have provided a detailed exposition of the simplex method from both primal and dual perspectives. Key points include:\nOnly basic feasible solutions (extreme points) need be considered. A change of basis (pivot) is effected by choosing an entering variable (with a negative reduced cost in the primal or a leaving variable in the dual) and then determining the corresponding leaving variable via a ratio test. Both the primal and dual simplex methods converge to an optimal solution when one exists, with the dual simplex method particularly useful when the initial basis is dual feasible. Under a basic value distribution assumption, a worst-case iteration bound can be derived. These foundational ideas underpin modern implementations of the simplex algorithm and continue to influence optimization theory and practice.\n","permalink":"https://xndrleib.github.io/posts/simplex/","summary":"\u003ch1 id=\"an-in-depth-overview-of-the-simplex-method\"\u003eAn In-Depth Overview of the Simplex Method\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAbstract\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis document presents an in-depth exposition of the simplex method for solving linear programs. We discuss fundamental concepts such as basic feasible solutions (BFS), the notion of adjacency between extreme points, and the nondegeneracy assumption. Both the primal and the dual simplex methods are described from a matrix–theoretic viewpoint with illustrative examples. Finally, we present a worst-case efficiency analysis under a \u003cem\u003ebasic value distribution\u003c/em\u003e assumption.\u003c/p\u003e","title":"Simplex"},{"content":"Hi!\n","permalink":"https://xndrleib.github.io/posts/blogpost/","summary":"\u003cp\u003eHi!\u003c/p\u003e","title":"My first blog post"}]